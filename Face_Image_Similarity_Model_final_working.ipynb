{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPpMLD7iVHlkPrjuE47aiqS"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# used to supress display of warnings\n",
        "import warnings\n",
        "\n",
        "from sklearn.metrics import precision_recall_curve,accuracy_score,f1_score,precision_score,recall_score"
      ],
      "metadata": {
        "id": "-UF4lqCQ7VQf"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pid-x6P97VNE",
        "outputId": "fea81dc9-c69b-44ba-bc14-0cb350a9f992"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# suppress display of warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "nX95UT177VKM"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "source_dir=os.path.join('/content','gdrive','MyDrive','105_classes_pins_dataset')"
      ],
      "metadata": {
        "id": "X34qYfwo7ua5"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class IdentityMetadata():\n",
        "    def __init__(self, base, name, file):\n",
        "        self.base = base\n",
        "        # identity name\n",
        "        self.name = name\n",
        "        # image file name\n",
        "        self.file = file\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.image_path()\n",
        "\n",
        "    def image_path(self):\n",
        "        return os.path.join(self.base, self.name, self.file)\n",
        "\n",
        "def load_metadata(path):\n",
        "    metadata = []\n",
        "    for i in os.listdir(path):\n",
        "        for f in os.listdir(os.path.join(path, i))[:50]:\n",
        "            # Check file extension. Allow only jpg/jpeg' files.\n",
        "            ext = os.path.splitext(f)[1]\n",
        "            if ext == '.jpg' or ext == '.jpeg':\n",
        "                metadata.append(IdentityMetadata(path, i, f))\n",
        "    return np.array(metadata)\n",
        "\n",
        "# metadata = load_metadata('images')\n",
        "metadata = load_metadata(source_dir)"
      ],
      "metadata": {
        "id": "oRvyIiot8RFm"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('metadata shape :', metadata.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gi04oPuU8RCJ",
        "outputId": "aab5de26-19ee-404e-8d08-e539a2653c5a"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "metadata shape : (5250,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metadata[150]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qsPEMDnu8Q_H",
        "outputId": "50f84655-7243-4587-ab1b-e820265797f5"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "/content/gdrive/MyDrive/105_classes_pins_dataset/pins_Tom Holland/Tom Holland104_4692.jpg"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(metadata[1500]), metadata[1500].image_path()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rHPCFPl08Q8i",
        "outputId": "a17ac270-1e8d-420c-f3e7-78dd1ee15a53"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(__main__.IdentityMetadata,\n",
              " '/content/gdrive/MyDrive/105_classes_pins_dataset/pins_Megan Fox/Megan Fox101_3368.jpg')"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "def load_image(path):\n",
        "    img = cv2.imread(path, 1)\n",
        "    # OpenCV loads images with color channels\n",
        "    # in BGR order. So we need to reverse them\n",
        "    return img[...,::-1]"
      ],
      "metadata": {
        "id": "ih2Gmx_u8uYM"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "load_image('/content/gdrive/MyDrive/105_classes_pins_dataset/pins_Ursula Corbero/Ursula Corbero244_110.jpg')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zCAObbSn8uUw",
        "outputId": "1f280562-2a40-426d-ac3f-d5f3175ae248"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[105, 196, 161],\n",
              "        [106, 193, 158],\n",
              "        [109, 188, 156],\n",
              "        ...,\n",
              "        [ 95, 180, 151],\n",
              "        [ 90, 175, 146],\n",
              "        [ 86, 170, 146]],\n",
              "\n",
              "       [[104, 194, 158],\n",
              "        [104, 190, 155],\n",
              "        [108, 185, 151],\n",
              "        ...,\n",
              "        [ 90, 175, 144],\n",
              "        [ 84, 169, 140],\n",
              "        [ 80, 164, 138]],\n",
              "\n",
              "       [[102, 186, 150],\n",
              "        [102, 182, 145],\n",
              "        [108, 180, 143],\n",
              "        ...,\n",
              "        [ 88, 173, 142],\n",
              "        [ 81, 164, 136],\n",
              "        [ 76, 159, 133]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[177, 101,  69],\n",
              "        [179, 103,  71],\n",
              "        [183, 104,  73],\n",
              "        ...,\n",
              "        [ 61,  36,  32],\n",
              "        [ 61,  36,  32],\n",
              "        [ 61,  36,  32]],\n",
              "\n",
              "       [[175,  98,  68],\n",
              "        [178, 101,  71],\n",
              "        [181, 102,  71],\n",
              "        ...,\n",
              "        [ 60,  35,  31],\n",
              "        [ 61,  36,  32],\n",
              "        [ 61,  36,  31]],\n",
              "\n",
              "       [[173,  96,  66],\n",
              "        [176,  99,  69],\n",
              "        [179, 100,  69],\n",
              "        ...,\n",
              "        [ 60,  35,  31],\n",
              "        [ 61,  36,  32],\n",
              "        [ 61,  36,  31]]], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import ZeroPadding2D, Convolution2D, MaxPooling2D, Dropout, Flatten, Activation\n",
        "\n",
        "def vgg_face():\n",
        "    model = Sequential()\n",
        "    model.add(ZeroPadding2D((1,1),input_shape=(224,224, 3)))\n",
        "    model.add(Convolution2D(64, (3, 3), activation='relu'))\n",
        "    model.add(ZeroPadding2D((1,1)))\n",
        "    model.add(Convolution2D(64, (3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
        "\n",
        "    model.add(ZeroPadding2D((1,1)))\n",
        "    model.add(Convolution2D(128, (3, 3), activation='relu'))\n",
        "    model.add(ZeroPadding2D((1,1)))\n",
        "    model.add(Convolution2D(128, (3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
        "\n",
        "    model.add(ZeroPadding2D((1,1)))\n",
        "    model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
        "    model.add(ZeroPadding2D((1,1)))\n",
        "    model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
        "    model.add(ZeroPadding2D((1,1)))\n",
        "    model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
        "\n",
        "    model.add(ZeroPadding2D((1,1)))\n",
        "    model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
        "    model.add(ZeroPadding2D((1,1)))\n",
        "    model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
        "    model.add(ZeroPadding2D((1,1)))\n",
        "    model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
        "\n",
        "    model.add(ZeroPadding2D((1,1)))\n",
        "    model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
        "    model.add(ZeroPadding2D((1,1)))\n",
        "    model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
        "    model.add(ZeroPadding2D((1,1)))\n",
        "    model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
        "\n",
        "    model.add(Convolution2D(4096, (7, 7), activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Convolution2D(4096, (1, 1), activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Convolution2D(2622, (1, 1)))\n",
        "    model.add(Flatten())\n",
        "    model.add(Activation('softmax'))\n",
        "    return model"
      ],
      "metadata": {
        "id": "PAENx8_LArV2"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = vgg_face()\n",
        "\n",
        "model.load_weights('/content/gdrive/MyDrive/vgg_face_weights.h5')"
      ],
      "metadata": {
        "id": "dApFIFnjEPpY"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Model\n",
        "vgg_face_descriptor = Model(inputs=model.layers[0].input, outputs=model.layers[-2].output)"
      ],
      "metadata": {
        "id": "nPKapbEBYrLf"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(vgg_face_descriptor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GU-asRaI9TPU",
        "outputId": "18a7ccd2-00d8-472d-bcc7-9a265689aa95"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "keras.src.engine.functional.Functional"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vgg_face_descriptor.inputs, vgg_face_descriptor.outputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNGHu4Y09WLy",
        "outputId": "b16beb17-307c-4eb5-9db5-f303acbf729c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([<KerasTensor: shape=(None, 224, 224, 3) dtype=float32 (created by layer 'zero_padding2d_input')>],\n",
              " [<KerasTensor: shape=(None, 2622) dtype=float32 (created by layer 'flatten')>])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get embedding vector for first image in the metadata using the pre-trained model\n",
        "img_path = metadata[0].image_path()\n",
        "img = load_image(img_path)\n",
        "\n",
        "# Normalising pixel values from [0-255] to [0-1]: scale RGB values to interval [0,1]\n",
        "img = (img / 255.).astype(np.float32)\n",
        "img = cv2.resize(img, dsize = (224,224))\n",
        "print(img.shape)\n",
        "\n",
        "# Obtain embedding vector for an image\n",
        "# Get the embedding vector for the above image using vgg_face_descriptor model and print the shape\n",
        "embedding_vector = vgg_face_descriptor.predict(np.expand_dims(img, axis=0))[0]\n",
        "print(embedding_vector.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hqe_-AA29X0K",
        "outputId": "1598e282-89cd-4ee7-e42e-e2fbe26966f5"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(224, 224, 3)\n",
            "1/1 [==============================] - 3s 3s/step\n",
            "(2622,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_vector[0], type(embedding_vector), type(embedding_vector[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "go-i6SuW9az7",
        "outputId": "b6578c66-2a61-459f-c292-02bed4afe141"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.015672842, numpy.ndarray, numpy.float32)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_vector[2], embedding_vector[98], embedding_vector[-2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SuAZLZIF9e54",
        "outputId": "b0bd5c76-91d2-45ad-f2eb-9f604e616066"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.0073238397, 0.0022011711, 0.0142918825)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_images = len(metadata)\n",
        "\n",
        "print('total_images :', total_images)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zubQSK-19gct",
        "outputId": "929413eb-39ae-4c97-aebc-cb839837754c"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total_images : 5250\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = np.zeros((metadata.shape[0], 2622))\n",
        "for i, m in enumerate(metadata):\n",
        "    img_path = metadata[i].image_path()\n",
        "    img = load_image(img_path)\n",
        "    img = (img / 255.).astype(np.float32)\n",
        "    img = cv2.resize(img, dsize = (224,224))\n",
        "    embedding_vector = vgg_face_descriptor.predict(np.expand_dims(img, axis=0),verbose=0)[0]\n",
        "    embeddings[i]=embedding_vector"
      ],
      "metadata": {
        "id": "clpqh4Cr9ipx"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings[5249]"
      ],
      "metadata": {
        "id": "BXOCGYrY9kxJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c7238cf-0020-4ff9-c8e6-634318c59305"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 2.05632485e-02, -3.10130417e-05,  1.21988868e-02, ...,\n",
              "       -4.87567158e-03,  3.28135933e-03,  7.03019742e-03])"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def distance(emb1, emb2, threshold=0.8):\n",
        "\n",
        "    # Calculate the distance between the embeddings\n",
        "    embedding_distance = emb1 - emb2\n",
        "\n",
        "    # Calculate the L2 norm of the distance vector\n",
        "    embedding_distance_norm = np.linalg.norm(embedding_distance)\n",
        "\n",
        "    # Return 1 if the distance is less than the threshold, else 0\n",
        "    return embedding_distance_norm if embedding_distance_norm < threshold else 0"
      ],
      "metadata": {
        "id": "e7fZdNrKQyhm"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def recognize(img_path):\n",
        "  img = load_image(img_path)\n",
        "  img = (img / 255.).astype(np.float32)\n",
        "  img = cv2.resize(img, dsize = (224,224))\n",
        "  embedding_vector = vgg_face_descriptor.predict(np.expand_dims(img, axis=0),verbose=0)[0]\n",
        "  distances = []\n",
        "  names = []\n",
        "  for i in range(len(embeddings)):\n",
        "    dist = distance(embedding_vector,embeddings[i])\n",
        "    if dist > 0:\n",
        "      distances.append(dist)\n",
        "      names.append(metadata[i].name)\n",
        "  if distances:\n",
        "    min_dist = min(distances)\n",
        "    return names[distances.index(min_dist)]\n",
        "\n",
        "  return \"No Match Found\""
      ],
      "metadata": {
        "id": "YzO0twq-ZTl0"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(recognize('/content/amanda_crew.jpg'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PbVnWTaLanGC",
        "outputId": "b1b67c06-ff67-4ae3-dc55-ef8a2f352437"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pins_Amanda Crew\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('fism.keras')"
      ],
      "metadata": {
        "id": "xVe95I-MNKus"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open('utils.pkl','wb') as f:\n",
        "  pickle.dump([embeddings,metadata],f)"
      ],
      "metadata": {
        "id": "NqaLwqLcesGs"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "new_model = load_model('/content/fism.keras')"
      ],
      "metadata": {
        "id": "9EN9pIa9Noe8"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imgt = load_image(metadata[5000].image_path())\n",
        "imgt = (imgt / 255.).astype(np.float32)\n",
        "imgt = cv2.resize(imgt, dsize = (224,224))\n",
        "et = vgg_face_descriptor.predict(np.expand_dims(imgt, axis=0),verbose=0)[0]"
      ],
      "metadata": {
        "id": "PJnOGY1BN3i5"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "et"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bub27V0Om3k",
        "outputId": "90711cfd-7fe8-4823-8fb6-55292276d817"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.00609213,  0.0066778 ,  0.01688961, ..., -0.02192893,\n",
              "        0.00516967,  0.01105241], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vwtTJuOKZi_b"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}